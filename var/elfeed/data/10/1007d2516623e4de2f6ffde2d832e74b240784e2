<html><head></head><body><p><em>Note: This article series was written by me, with LLMs helping to refine the style and structure.</em></p>
<p>Working with 10 parallel LLM coding sessions exposes problems that don’t appear at smaller scale. Managing multiple conversations, maintaining context across sessions, and ensuring quality all require different approaches than single-session work.</p>
<p>This series documents those problems and the solutions that emerged. The tools shown use Claude Code and Emacs, but the patterns apply broadly to any LLM workflow.</p>
<p><strong>Series Navigation:</strong> <a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part2-ergonomics">Part 2: Ergonomics →</a></p>
<h2>
  The Pain Points
  <a href="#the-pain-points">
    <i></i>
  </a>
</h2>
<p>The problems:</p>
<ol>
<li><strong><a href="#problem-1-managing-multiple-conversations">Managing Multiple Conversations</a></strong> - 10 terminal windows, no visibility into which sessions need attention</li>
<li><strong><a href="#problem-2-lost-context">Lost Context</a></strong> - No audit trail of past sessions or decisions made</li>
<li><strong><a href="#problem-3-quality-and-regressions">Quality &amp; Regressions</a></strong> - LLMs fix one thing, break another</li>
<li><strong><a href="#problem-4-language-specific-edit-challenges">Language-Specific Edit Challenges</a></strong> - Parenthesis balance issues in Lisp</li>
<li><strong><a href="#problem-5-project-exploration-speed">Project Exploration Speed</a></strong> - 10+ minutes to load a 20-file project</li>
<li><strong><a href="#problem-6-context-switching-between-sessions">Context Switching Between Sessions</a></strong> - No shared knowledge between parallel sessions</li>
<li><strong><a href="#problem-7-review-without-full-ide-context">Review Without Full IDE Context</a></strong> - Reviewing diffs without syntax highlighting and jump-to-def</li>
<li><strong><a href="#problem-8-no-long-term-memory">No Long-Term Memory</a></strong> - Every session starts from scratch</li>
<li><strong><a href="#problem-9-parallelization-challenge">Parallelization Challenge</a></strong> - Coordinating multiple LLMs working simultaneously</li>
<li><strong><a href="#problem-10-safety-and-access-control">Safety and Access Control</a></strong> - Too easy to grant access to private data</li>
</ol>
<p>Let’s dive into each of these.</p>
<h2>
  Problem 1: Managing Multiple Conversations
  <a href="#problem-1-managing-multiple-conversations">
    <i></i>
  </a>
</h2>
<p>Picture this: 10 terminal windows, each running a different LLM session. One is refactoring your note system, another is debugging a home automation script, a third is implementing a new feature. Zero visibility into which needs your attention.</p>
<p>The problem becomes clear when context switching:</p>
<ul>
<li>Which session is waiting for input?</li>
<li>Which is still processing?</li>
<li>Which finished 10 minutes ago and has been idle?</li>
</ul>
<p>Without state tracking across sessions, every context switch means manually checking each window. You switch to a session only to find the LLM finished 10 minutes ago while you were focused elsewhere.</p>
<h2>
  Problem 2: Lost Context
  <a href="#problem-2-lost-context">
    <i></i>
  </a>
</h2>
<p>Open a project you worked on last week with an LLM. The code looks unfamiliar. You don’t remember writing it. Questions arise:</p>
<ul>
<li>What was the original prompt?</li>
<li>Did I review this properly?</li>
<li>What architectural decisions were made?</li>
<li>Why this approach instead of alternatives?</li>
</ul>
<p>Without an audit trail of past sessions, there’s no way to reconstruct the reasoning behind the code. You’re essentially trusting that past-you made good decisions—but you have no record of what those decisions were.</p>
<p>Automatic context compaction makes this worse. LLMs will drop older messages to fit within token limits, but I want explicit control over what gets retained from session to session, not an algorithm deciding what’s “important.”</p>
<h2>
  Problem 3: Quality and Regressions
  <a href="#problem-3-quality-and-regressions">
    <i></i>
  </a>
</h2>
<p>Whack-a-mole development: LLMs fix one issue and silently break another. The problem wasn’t the LLM’s capabilities—it was my process. I was treating LLM sessions like conversations with a developer I trusted to test their own code.</p>
<p>The first solution: treat every change like a pull request. Tests must pass.</p>
<div class="highlight"><pre><code class="language-bash"><span><span><span># After every LLM change</span>
</span></span><span><span>make <span>test</span>  <span># Must pass before continuing</span>
</span></span></code></pre></div><p>This catches regressions but doesn’t solve architectural consistency. Code generated across dozens of separate sessions felt scattered, like it was designed by committee where no one talked to each other.</p>
<p>The second solution: persona-based prompts. Instead of “Refactor this code”:</p>
<pre><code>You are Robert C. Martin (Uncle Bob). Review this code and refactor
it according to clean code principles.
</code></pre><p>The difference was striking. Suddenly: smaller functions, better separation of concerns, consistent naming conventions across the codebase.</p>
<p>You can use different personas for different needs. Want paranoid security review? “You are a security-minded, paranoid QA engineer who trusts nothing.” Need simplicity? “You are obsessed with reducing complexity and eliminating unnecessary abstractions.” The persona focuses the LLM’s attention on specific concerns.</p>
<h2>
  Problem 4: Language-Specific Edit Challenges
  <a href="#problem-4-language-specific-edit-challenges">
    <i></i>
  </a>
</h2>
<p>Lisp-based languages (Elisp, Clojure, Scheme) are harder for LLMs to edit because of parenthesis balance.</p>
<p>The problem: Remove one closing paren and get “end-of-file during parsing” with no location. The error could be 200 lines away from the actual edit.</p>
<p>The feedback loop:</p>
<ol>
<li>LLM edits code</li>
<li>Compile fails</li>
<li>Hunt for unbalanced paren manually</li>
<li>Fix and retry</li>
</ol>
<p>This affects any language with nested structure spanning many lines: deeply nested JSON, XML, etc.</p>
<p>The solution: validation tooling that gives precise error locations. Without that, you’re debugging blind.</p>
<h2>
  Problem 5: Project Exploration Speed
  <a href="#problem-5-project-exploration-speed">
    <i></i>
  </a>
</h2>
<p>New codebase? Get ready to spend 10+ minutes on initial exploration. A 20-file project means feeding files one by one to the LLM, waiting for API calls, managing context windows.</p>
<p>This creates a cold-start problem. Every new project or every time you switch projects means a lengthy ramp-up period before the LLM has enough context to be productive.</p>
<p>The solution: a way to efficiently snapshot and load project context—not just individual files, but the structure, key patterns, and architectural decisions all at once.</p>
<h2>
  Problem 6: Context Switching Between Sessions
  <a href="#problem-6-context-switching-between-sessions">
    <i></i>
  </a>
</h2>
<p>I’d discover a great pattern in session A. Session B, working on a related problem, had no idea it existed.</p>
<p>Each LLM conversation was an island. Problems with this isolation:</p>
<ul>
<li>Can’t share knowledge between sessions</li>
<li>Contradictory decisions across different LLM instances</li>
<li>Manual copy-paste required to propagate learnings</li>
<li>If I made an architectural decision in conversation A, conversation B would make a different one</li>
</ul>
<p>The solution: a shared context system where different LLM sessions can coordinate and learn from each other.</p>
<h2>
  Problem 7: Review Without Full IDE Context
  <a href="#problem-7-review-without-full-ide-context">
    <i></i>
  </a>
</h2>
<p>Code review without your IDE is code review on hard mode.</p>
<p>The LLM generates a diff. You’re looking at it in a terminal or web interface. You’re missing:</p>
<ul>
<li>Syntax highlighting</li>
<li>Jump-to-definition</li>
<li>Project-wide search</li>
<li>Static analysis</li>
<li>Your configured linters</li>
</ul>
<p>Example: The LLM renames <code>process()</code> to <code>process_data()</code>. Questions you can’t answer:</p>
<ul>
<li>What calls this function?</li>
<li>Is this part of a larger refactoring?</li>
<li>Did it affect other functions that depend on it?</li>
</ul>
<p>Tools like Cursor solve this with deep editor integration—the LLM changes happen natively in your IDE. But if you’re using terminal-based LLM tools or trying to integrate with Emacs/Vim, you need a workflow to bring LLM-generated changes into your full development environment.</p>
<h2>
  Problem 8: No Long-Term Memory
  <a href="#problem-8-no-long-term-memory">
    <i></i>
  </a>
</h2>
<p>Sessions had amnesia. Yesterday’s architectural decisions? Gone. Last week’s patterns? Forgotten.</p>
<p>Sure, I had a global CLAUDE.md file with preferences, but that was static. I couldn’t easily capture evolving patterns like:</p>
<ul>
<li>“When working on MCP servers, always check the umcp wrapper patterns”</li>
<li>“The smoke test paradigm works better than unit tests for these projects”</li>
<li>“Remember that the memento CLI should never be called directly—use MCP”</li>
</ul>
<p>These insights lived in my head, not in a form the LLM could access and build upon. Each new session started from zero, unable to leverage the accumulated knowledge from previous sessions.</p>
<h2>
  Problem 9: Parallelization Challenge
  <a href="#problem-9-parallelization-challenge">
    <i></i>
  </a>
</h2>
<p>I wanted parallel LLM sessions building different parts of the same project. Chaos ensued.</p>
<p>The ideal workflow:</p>
<ul>
<li>Session A: implements a feature</li>
<li>Session B: writes tests for that feature</li>
<li>Session C: updates documentation</li>
<li>Session D: reviews the changes from A, B, and C</li>
</ul>
<p>But coordinating multiple LLM sessions is harder than coordinating humans. Problems:</p>
<ul>
<li>Sessions can’t see each other’s progress</li>
<li>No natural communication channel between sessions</li>
<li>They’ll happily work on the same file and create conflicts</li>
<li>No way to express dependencies (Session B needs Session A to finish first)</li>
</ul>
<p>The solution: orchestration patterns to divide tasks, prevent conflicts, and merge results without manual intervention.</p>
<h2>
  Problem 10: Safety and Access Control
  <a href="#problem-10-safety-and-access-control">
    <i></i>
  </a>
</h2>
<p>When you’re in flow, you say ‘yes’ to everything. That’s how the LLM reads your private notes.</p>
<p>Claude Code prompts have become like cookie consent banners or Terms of Service pages. You’ve seen the prompt 50 times today. “Do you want to let Claude read this file?” Yes. “Run this command?” Yes. “Search this directory?” Yes. Decision fatigue sets in. You stop reading carefully. You just click yes to make the prompt go away and get back to work.</p>
<p>This is exactly how website designers exploit users with cookie banners—they know after the 10th website, you’ll just click “Accept All” without reading. The same psychological pattern applies to LLM tool use.</p>
<p>I discovered a serious problem when building my note management system. Despite explicit prompts telling the LLM “do NOT access private notes,” I’d occasionally review logs and find it had read private files anyway. This wasn’t malicious—the LLM was trying to be helpful, pattern-matched similar file paths, and I’d reflexively approved the request without carefully reading which specific file it wanted.</p>
<p>Risk areas where this becomes dangerous:</p>
<ul>
<li>Personal notes or journals</li>
<li>Configuration files with API keys or tokens</li>
<li>Any sensitive data mixed with development work</li>
</ul>
<p>The fundamental tension:</p>
<ul>
<li><strong>Speed vs Safety</strong>: Careful review of every action slows you down</li>
<li><strong>Context vs Control</strong>: The LLM needs broad context to be useful, but that increases risk</li>
<li><strong>Automation vs Oversight</strong>: You want automated workflows, but automation can bypass safety checks</li>
</ul>
<p>The real solution isn’t better logging—it’s making the wrong thing impossible by design. Don’t rely on prompts or careful review. Build systems where sensitive data simply can’t be accessed.</p>
<p>For my note system, I mark notes as PUBLIC in org-mode by setting a property. Only PUBLIC notes are accessible to the LLM via MCP. The system enforces this at the API level—no amount of prompt engineering or reflexive approval can expose private notes.</p>
<p>But this pattern doesn’t scale well to code. You can’t mark every file in a codebase as PUBLIC or PRIVATE.</p>
<p>A more scalable approach: leverage Unix file permissions. Make LLM tools run as a specific user or group with restricted permissions:</p>
<ul>
<li>Private files: <code>chmod 600</code> (owner-only)</li>
<li>Public files: <code>chmod 644</code> (world-readable)</li>
<li>LLM runs as different user/group: physically cannot read private files</li>
</ul>
<p>This enforces access control at the OS level. The LLM tool literally can’t open the file, regardless of prompts or approval. You could even use <code>chattr +i</code> on Linux to make sensitive files immutable.</p>
<p>The challenge: this requires discipline in setting permissions and may conflict with normal development workflows. But it’s the right direction—making violations impossible, not just logged.</p>
<p>Other needed patterns:</p>
<ul>
<li>Directory-level access control (allow <code>~/projects/blog</code>, block <code>~/.ssh</code>)</li>
<li>Pattern-based restrictions (block <code>*.env</code>, <code>*credentials*</code>, <code>*secrets*</code>)</li>
<li>API-level enforcement that tools can’t bypass</li>
<li>Audit trails that make violations obvious</li>
</ul>
<p>Until we solve this systematically, the onus is on us to be vigilant—and that’s exhausting when you’re trying to move fast.</p>
<h2>
  The Solutions
  <a href="#the-solutions">
    <i></i>
  </a>
</h2>
<ol>
<li><strong>Ergonomics</strong> (<a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part2-ergonomics">Part 2</a>): Terminal integration showing LLM state, telemetry tracking all sessions, logging every command</li>
<li><strong>Abstractions</strong> (<a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part3-abstractions">Part 3</a>): Shared context between sessions, smoke test paradigm, coordinating parallel LLMs</li>
<li><strong>Experiments</strong> (<a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part4-experiments">Part 4</a>): Project exploration tools, diff review workflows, lessons from failures</li>
<li><strong>Learning</strong> (<a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part5-learning">Part 5</a>): Flashcard generation, annotated code worksheets, spaced repetition</li>
</ol>
<p>The next articles show how each works.</p>
<h2>
  What’s Next
  <a href="#whats-next">
    <i></i>
  </a>
</h2>
<p><strong><a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part2-ergonomics">Part 2: Ergonomics and Observability</a></strong> - Terminal integration for managing multiple LLM sessions, telemetry and logging infrastructure that makes everything auditable.</p>
<p><strong><a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part3-abstractions">Part 3: Higher-Level Abstractions</a></strong> - Shared context systems for long-term memory, smoke tests as the foundation of quality, patterns for coordinating multiple LLM sessions.</p>
<p><strong><a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part4-experiments">Part 4: The Way We Build Software Is Rapidly Evolving</a></strong> - Tools that became obsolete, workflows that work, and the broader implications of AI-augmented development.</p>
<p><strong><a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part5-learning">Part 5: Learning &amp; Knowledge</a></strong> - Using LLMs to generate flashcards, worksheets, and heavily-annotated code for studying complex topics.</p>
<hr>
<p><strong>Continue Reading:</strong> <a href="https://blog.laurentcharignon.com/post/2025-09-30-llm-workflow-part2-ergonomics">Part 2: Ergonomics and Observability →</a></p>
</body></html>