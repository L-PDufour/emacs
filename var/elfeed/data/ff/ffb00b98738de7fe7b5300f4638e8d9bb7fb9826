<div><img width="300" height="234" src="https://blog.sigplan.org/wp-content/uploads/2024/11/shutterstock_2414345055-scaled-e1732129274437-300x234.jpg" class="attachment-medium size-medium wp-post-image" alt="" style="margin-bottom:15px;margin-left:15px;float:right;" decoding="async" loading="lazy" /></div><p><span style="font-weight: 400">Part of PL research is building a science of programming. PL has a science of program correctness: we make precise claims about soundness / completeness / undefined behavior / etc., and we justify those claims with proofs. PL has a science of program performance: we make precise claims about relative speed or resource utilization compared to prior art, and we justify those claims with benchmarks. </span></p>
<p><span style="font-weight: 400">PL researchers also care about program usability. Foundational ideas in our field are predicated on beliefs about people: </span></p>
<ul>
<li style="font-weight: 400"><b>Compilers</b><span style="font-weight: 400">: “We were after getting correct programs written faster, and getting answers for people faster” </span><a href="https://dl.acm.org/doi/10.1145/800025.1198341"><span style="font-weight: 400">(Grace Hopper)</span></a></li>
<li style="font-weight: 400"><b>Structured programming</b><span style="font-weight: 400">: “Our intellectual powers are rather geared to master static relations and that our powers to visualize processes evolving in time are relatively poorly developed” </span><a href="https://dl.acm.org/doi/10.1145/362929.362947"><span style="font-weight: 400">(Edsger Dijkstra)</span></a></li>
<li style="font-weight: 400"><b>Profiling</b><span style="font-weight: 400">: “We think we know what programmers generally do but our notions are rarely based on a representative sample of the programs which are actually run on computers” </span><a href="https://onlinelibrary.wiley.com/doi/10.1002/spe.4380010203"><span style="font-weight: 400">(Donald Knuth)</span></a></li>
<li style="font-weight: 400"><b>Language design</b>: “The primary purpose of a programming language is to help the programmer in the practice of his art” <a href="https://dl.acm.org/doi/10.5555/892013">(Tony Hoare)</a></li>
</ul>
<p><span style="font-weight: 400">This observation still rings true today – one can find dozens of examples in every PL venue (even POPL!) describing systems or techniques as “usable”, “intuitive”, “easy to reason about,” and so on. However, the thesis of this post is that PL research lacks a commensurate science of human factors for dealing with these claims. PL papers tend to make imprecise usability claims and justify them with weak methods like comparing lines of code (LOC). The goal of this post is to provide actionable advice for PL researchers to improve their human-centered vocabulary and methods. Rather than just saying “run more user studies”, I will argue to embrace qualitative methods, modernize usability metrics, and leverage prior work on programmer psychology.</span></p>
<h2><span style="font-weight: 400">User Studies Cannot Be The (Only) Answer</span></h2>
<p><span style="font-weight: 400">I am not the first person to argue for a more human-centered approach to PL. </span><a href="https://ieeexplore.ieee.org/document/7503516"><span style="font-weight: 400">“Programmers Are Users Too”</span></a><span style="font-weight: 400"> and </span><a href="https://cacm.acm.org/research/pl-and-hci/"><span style="font-weight: 400">“PL and HCI: Better Together”</span></a><span style="font-weight: 400"> are excellent articles that advocate for importing ideas from the field of human-computer interaction (HCI) into PL research, such as user studies, iterative design, and contextual inquiry. These techniques are valuable; I use them in my own research on languages with sizable populations of existing users, such as Rust.</span></p>
<p><span style="font-weight: 400">However, these techniques are often impractical for PL research. Consider trying to run a user study:</span></p>
<ol>
<li style="font-weight: 400"><span style="font-weight: 400">You need to get approval from an ethics board. If it’s less than two months before your conference deadline, then it’s probably too late!</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">You need to recruit participants. Consider any given niche of PL: cubical type theory, Racket macrology, e-graph optimizers, or quantum computing. There may be at most 100 people in the world who are qualified to understand your tool.</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">You need to teach participants your tool. But no one meaningfully picks up complex languages like Haskell or Coq in a day. Do you have the money to pay for world-class programmers to learn your tool for potentially a week or more?</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">You need to have your participants stress-test the conceptually important aspects of the tool. But more likely, they’ll get hung up on the “front-line UX”: bad error messages, confusing documentation, and so on. And what if your conceptual contribution is most apparent in large codebases? Then your participants need to learn the large codebase first! (How big is your grant again?)</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">Hopefully you find a positive result at the end!</span></li>
</ol>
<p><span style="font-weight: 400">A relevant parable comes from the systems community: the Raft consensus protocol. The principal design goal of Raft is reflected in its paper title, </span><a href="https://www.usenix.org/conference/atc14/technical-sessions/presentation/ongaro"><span style="font-weight: 400">“In Search of an Understandable Consensus Algorithm”</span></a><span style="font-weight: 400">. Diego Ongaro and John Ousterhout felt that existing algorithms (namely Paxos) were too complicated for most people to understand, so they invented an algorithm that they believed was more understandable. This belief has arguably borne out in practice. Many production-grade distributed systems have adopted Raft in part due to its perceived understandability.</span></p>
<p><span style="font-weight: 400">Yet, Ongaro and Ousterhout struggled to convince program committees. Their initial paper relied on qualitative justifications, and was repeatedly rejected from NSDI and SOSP for lack of a sufficiently rigorous evaluation. The pair eventually did a user study that involved teaching Raft and Paxos to students at Berkeley and Stanford, and then comparing the algorithms&#8217; understandability via scores on a post-lecture exam. Ousterhout later <a href="https://www.youtube.com/watch?v=PfE8P4nwcY4">said of the experience</a>:</span></p>
<blockquote><p><span style="font-weight: 400">“User studies are [&#8230;] a huge amount of work, and you don’t know whether it’s going to work until you reach the very end. [&#8230;] We couldn’t tell whether the results were going to favor Raft or Paxos. And we were wondering, what are we going to do if Paxos wins? Do we cancel the project at this point?”</span></p></blockquote>
<p><span style="font-weight: 400">Thankfully, they found a small but positive result. Test scores were 8% higher with Raft than Paxos. </span><i><span style="font-weight: 400">And even still</span></i><span style="font-weight: 400">, PCs were unimpressed — the study was lambasted as unscientific, non-neutral, and too small of a sample size. Ousterhout remarked that the ultimate value of the study was that it “made the PCs stop complaining about a lack of evaluation.”</span></p>
<p><span style="font-weight: 400">What can we learn from this parable? My takeaway is that these kinds of user studies are functionally useless. They consume an enormous amount of researcher time and energy. Their results are shallow — an 8% increase in test scores would not predict Raft’s substantial impact in practice. But the culprits here are not Ongaro and Ousterhout, but rather the culture in software systems research that refuses any other form of evidence for a human-centered claim. We need to develop better standards for usability evaluation so that the next Raft does not die on the vine when its last-ditch user study goes wrong.</span></p>
<h2><span style="font-weight: 400">Making “Usability” Precise</span></h2>
<p><span style="font-weight: 400">A good place to start is </span><i><span style="font-weight: 400">vocabulary</span></i><span style="font-weight: 400">: how can we articulate human-centered claims? For example, we would not generally accept a PL paper that merely claims a type system is “correct.” We expect a more precise statement, such as “sound with respect to undefined behavior”. However, there are myriad examples of PL papers that make claims about a system’s “usability”, point blank. Here’s a selection covering every major PL conference just in the past year:</span></p>
<ul>
<li style="font-weight: 400"><span style="font-weight: 400">“</span><b>Usability</b><span style="font-weight: 400"> of quotient types in a general purpose language was the guiding principle in the design choices of Quotient Haskell” </span><a href="https://dl.acm.org/doi/10.1145/3632869"><span style="font-weight: 400">(Hewer and Hutton, POPL 2024)</span></a></li>
<li style="font-weight: 400"><span style="font-weight: 400">“We demonstrate the </span><b>usability</b><span style="font-weight: 400"> and versatility of the [JavaScript regular expression] mechanization through a broad collection of analyses, case studies, and experiments” </span><a href="https://dl.acm.org/doi/10.1145/3674666"><span style="font-weight: 400">(De Santo et al., ICFP 2024)</span></a></li>
<li style="font-weight: 400"><span style="font-weight: 400">“We plan to enhance [our language’s] </span><b>usability</b><span style="font-weight: 400"> by not requiring the user to specify the hyperparameters for every probabilistic program.” </span><a href="https://dl.acm.org/doi/10.1145/3656412"><span style="font-weight: 400">(Garg et al., PLDI 2024)</span></a></li>
<li style="font-weight: 400"><span style="font-weight: 400">“To demonstrate the </span><b>usability</b><span style="font-weight: 400"> and maturity of Byods, we implemented a realistic whole-program pointer analysis for LLVM IR” </span><a href="https://dl.acm.org/doi/10.1145/3622840"><span style="font-weight: 400">(Sahebolamri et al., OOPSLA 2023)</span></a></li>
<li style="font-weight: 400"><span style="font-weight: 400">“To improve the </span><b>usability</b><span style="font-weight: 400"> of Allo, we have implemented the frontend language in Python” <a href="https://dl.acm.org/doi/10.1145/3656401">(Chen et al., PLDI 2024)</a></span></li>
</ul>
<p><span style="font-weight: 400">In these quotes, usability contains many shades of meaning:</span></p>
<ol>
<li style="font-weight: 400"><span style="font-weight: 400">Usability as “able to be used”: literally that a system works, and is not just theoretical.</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">Usability as “able to be used in realistic contexts”: that a system can work, with enough effort, for a realistic type/scale of problem.</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">Usability as “easy to be used by people”: that a person can work with the system productively, or that it has a low learning curve.</span></li>
</ol>
<p><span style="font-weight: 400">I believe we should separate the first two meanings from the third. “Able to be used [in realistic contexts]” is not really a human-centered claim as much as a logistical one, whereas “easy to be used by people” is a distinct category. I will propose that the first two meanings would be better called </span><i><span style="font-weight: 400">practicality</span></i><span style="font-weight: 400"> than </span><span style="font-weight: 400">usability</span><span style="font-weight: 400">.</span></p>
<p><span style="font-weight: 400">For the human-centered definition of usability, we should use more precise words to delineate why a system is more usable. For example, say the usability claim is “people used to have to manually specify a thing, but now we can infer that thing for them” (as in the Quotient Haskell and probabilistic programming examples). The reason this improves usability depends on why the user previously needed to specify the thing. Is the new system providing “smart default”, i.e. a reasonable but not-guaranteed-perfect choice? Or is the new system providing a complete substitute that the user never needs to reason about? In the former case, I would say the system is reducing </span><i><span style="font-weight: 400">premature commitment </span></i><span style="font-weight: 400">(the need to provide details before necessary), while the latter is reducing </span><i><span style="font-weight: 400">diffuseness</span></i><span style="font-weight: 400"> (the amount of notation needed to express an idea).</span></p>
<p><span style="font-weight: 400">These concepts of premature commitment and diffuseness are just two of the many </span><a href="https://www.sciencedirect.com/science/article/pii/S1045926X96900099"><span style="font-weight: 400">cognitive dimensions of notation</span></a><span style="font-weight: 400">, a set of criteria for evaluating the usability of programming tools derived from programming psychology experiments</span><span style="font-weight: 400">. The cognitive dimensions are not perfect or exhaustive, but they are a good starting point. I strongly encourage PL researchers to read the linked paper and consider adopting its vocabulary in lieu of general statements about usability. </span><span style="font-weight: 400">If you want concrete examples of papers using the cognitive dimensions, check out &#8220;Designing the Whyline&#8221; (<a href="https://dl.acm.org/doi/abs/10.1145/985692.985712">Ko and Myers 2004</a>) and &#8220;Protovis: A Graphical Toolkit for Visualization&#8221; (<a href="https://ieeexplore.ieee.org/abstract/document/5290720">Bostock and Heer 2009</a>)</span><span style="font-weight: 400">.</span></p>
<p><span style="font-weight: 400">Additionally, most usability claims need to be qualified with the expected audience. For instance, “we have implemented the frontend language in Python” only makes a tool more usable to people who know Python. (This raises the more interesting question: why pick Python?) If you want to make a usability claim, consider whether your system might be used differently along lines like these:</span></p>
<ul>
<li style="font-weight: 400"><span style="font-weight: 400">Novice programmers vs. expert programmers</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">Application developers vs. library developers vs. language developers</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">People working in small teams vs. people working in large teams</span></li>
<li style="font-weight: 400"><span style="font-weight: 400">People working in open-source vs. people working in companies</span></li>
</ul>
<p><span style="font-weight: 400">Before moving on, I want to briefly comment on some other questionable words:</span></p>
<p><b>Intuitive: </b><span style="font-weight: 400">I have been </span><a href="https://twitter.com/tonofcrates/status/1274407812144095233"><span style="font-weight: 400">on the record</span></a><span style="font-weight: 400"> against this word for a while. At this point, I am convinced that no one in the PL community actually knows what it means. Let me quote Merriam-Webster:</span></p>
<blockquote><p><i><span style="font-weight: 400">intuition</span></i><span style="font-weight: 400">: the power or faculty of attaining to direct knowledge or cognition without evident rational thought and inference</span></p></blockquote>
<p><span style="font-weight: 400">I will assert there is very little in PL research that one can deduce without rational thought or inference. However, I am not focusing heavily on “intuitive” because it is less commonly used to describe systems than to describe steps in proofs. I believe what authors usually mean is “informally”, “abstractly”, “in summary”, or “at a high level”.</span></p>
<p><b>Complex:</b><span style="font-weight: 400"> I suspect that researchers use this word far too freely (89% of papers last year used it at least once). Putting aside its use as a term of art (“computational complexity”), there are few intrinsic measures of complexity (e.g., of a system, concept, or proof). Complexity is in the eye of the beholder, but papers are rarely explicit about who is the beholder.</span></p>
<h2><span style="font-weight: 400">Thinking Past Lines of Code</span></h2>
<p><span style="font-weight: 400">Imagine you were the first person to invent the concepts of map, filter, and reduce. You probably believe these functions are useful, i.e., they make list processing more usable than C-style for-loops / if-statements or ML-style inductive functions. How would you justify that claim? In a PL paper today, you might find a subsection of the evaluation that looks like this:</span></p>
<blockquote><p><span style="font-weight: 400">“On a benchmark of 10 list processing tasks, we implemented each task in C and in Lambda-C. The C programs were on average 15 lines long, while the Lambda-C programs were on average 5 lines long. Therefore, the Lambda-C programs are 66% more usable.”</span></p></blockquote>
<p><span style="font-weight: 400">This is a caricature, but it is uncomfortably close to reality. Software systems research frequently relies on LOC as a proxy for every human-centered attribute of a system: usability, complexity, difficulty, and so on. Depending on the venue, ¼ to ¾ of (distinguished!) PL/SE/systems papers involve measurements of code size <a href="https://dl.acm.org/doi/abs/10.1145/3426428.3426921">(Alpernas et al. 2020)</a></span><span style="font-weight: 400">. It is a symptom of the problem observed by Ousterhout — qualitative evaluation is often not perceived as comparably rigorous to quantitative evaluation, so authors feel pressure to publish some kind of number to back up a usability claim. </span></p>
<p><span style="font-weight: 400">The challenge for PL researchers is to identify evaluation methods with a high return on investment. User studies can (sometimes) provide significant returns in rigor, but the investment is also significant. Measuring code size is low investment, but it provides few returns beyond the illusion of rigor.</span></p>
<p><span style="font-weight: 400">I believe the first step forward is that </span><b>PL/SE/systems researchers must embrace qualitative evaluation. </b><span style="font-weight: 400">I’m referring to arguments based on design principles and case studies rather than numeric data. For instance, John Backus provided a powerful set of qualitative arguments for combinator-style data processing in his <a href="https://dl.acm.org/doi/pdf/10.1145/359576.359579">Turing lecture</a></span><span style="font-weight: 400">. I could rephrase a few of his arguments using the cognitive dimensions:</span></p>
<ul>
<li><b>Closeness of mapping</b><span style="font-weight: 400">: if your task is “do <code>f</code> to each element of a list”, then <code>map f</code> describes that task more directly than the equivalent for-loop. For example, it should be easier for me to formally prove that <code>map f</code> implements the task than the loop-style program.</span></li>
<li><b>Error-proneness:</b><span style="font-weight: 400"> if you use an indexed for-loop (e.g., <code>for(int i = 0; i &lt; N; i++) { ... }</code>) then you could easily slip up: <code>i</code> could be initialized to the wrong number, <code>N</code> could refer to the wrong length, <code>i++</code> could be the wrong step, or you could use the wrong variable when indexing (e.g., <code>a[j]</code>). Combinators reduce the number of such errors.</span></li>
<li><b>Role-expressiveness</b><span style="font-weight: 400">: when a programmer reads a <code>map f</code>, it is clearer at a glance what it’s doing than an equivalent for-loop.</span></li>
</ul>
<p><span style="font-weight: 400">When this kind of analysis is performed systematically across a carefully-selected range of examples, it should be considered a legitimate form of evaluation. Reviewers should not categorically require quantitative evidence as a precondition for publication. It’s equally important that communities develop shared standards for qualitative evaluation. Shared standards help authors design better evaluations, and they help reviewers more constructively critique those evaluations. Documents like the </span><a href="https://www.sigplan.org/Resources/EmpiricalEvaluation/"><span style="font-weight: 400">SIGPLAN Empirical Evaluation Guidelines</span></a><span style="font-weight: 400"> should be modernized to reflect best practices for performing qualitative evaluations such as case studies. </span></p>
<p>I would argue that effective case study analysis includes characteristics like <em>diverse selection</em> (picking a reasonable and representative set of examples), <em>detailed comparison</em> (analyzing a case study in depth, e.g., giving &#8220;<a href="https://en.wikipedia.org/wiki/Thick_description">thick description</a>&#8220;), and <em>distinct insights</em> (generating ideas that specifically need qualitative rather than quantitative analysis). For example, these are some recent PL papers that embody these characteristics:</p>
<ul>
<li>&#8220;Capturing Types&#8221; (<a href="https://dl.acm.org/doi/full/10.1145/3618003">Boruch-Gruszecki et al. 2024</a>) shows how to evaluate the smoothness of a transition to an extended type system.</li>
<li>&#8220;Associated Effects&#8221; (<a href="https://dl.acm.org/doi/10.1145/3656393">Lutze and Madsen 2024</a>) shows how to build up increasingly complex examples that demonstrate type-level expressiveness against related work.</li>
<li>&#8220;The Ultimate Conditional Syntax&#8221; (<a href="https://dl.acm.org/doi/10.1145/3689746">Cheng and Parreaux 2024</a>) shows how to deeply compare against a wide range of related work.</li>
</ul>
<h2><span style="font-weight: 400">Improving Usability Metrics</span></h2>
<p><span style="font-weight: 400">I don’t want to come across as saying that quantitative usability metrics like LOC are useless. For example, say that two systems do the same thing, but one is 100 LOC while the other is 10,000 LOC. Without any further information, most developers would guess that the 100 LOC system is more usable, at least along certain dimensions like readability. </span></p>
<p><span style="font-weight: 400">The problem is that LOC is a very lossy proxy for usability. For example, a 2021 study evaluated code complexity metrics by comparing them to people’s subjective reports and objective measures of brain activity via fMRI </span><a href="https://ieeexplore.ieee.org/document/9402005"><span style="font-weight: 400">(Peitek et al. 2021)</span></a><span style="font-weight: 400">. Every evaluated metric, including LOC, had at best a small correlation with both subjective and objective complexity. So if two systems are 600 vs. 800 LOC, it’s not obvious that the 600 LOC system is 25% more usable.</span></p>
<p><span style="font-weight: 400">My proposed second step forward is that </span><b>researchers should develop new usability metrics that better map onto human experience. </b><span style="font-weight: 400">For example, rather than rejecting LOC comparisons entirely, we should only accept LOC comparisons that are </span><i><span style="font-weight: 400">very likely</span></i><span style="font-weight: 400"> to indicate a difference in code complexity. Here’s a serious proposal for one way to do that. In the study of human perception, there’s a concept called <a href="https://dictionary.apa.org/fechners-law">Fechner’s law</a>:</span></p>
<blockquote><p><span style="font-weight: 400">“Sensation experienced is proportional to the logarithm of the stimulus magnitude”</span></p></blockquote>
<p><span style="font-weight: 400">You may have encountered this concept in sound measurements. Audio decibels are a logarithmic scale, because a sound that is objectively twice as loud is not always </span><i><span style="font-weight: 400">perceived</span></i><span style="font-weight: 400"> as twice as loud. This law actually holds for all the senses: sight, sound, touch, smell, and taste.</span></p>
<p><span style="font-weight: 400">I claim that it is useful to conceptualize code size on a logarithmic scale because each step of the scale corresponds to meaningfully different levels of complexity. 1 LOC is a single action. 10 LOC is your average function. 100 LOC is an average class. 1,000 LOC is a library. 10,000 is a small application. And so on. In terms of decibel lines of code (call it “deciloc”, or “dLOC”), those are 0, 10, 20, 30, and 40 dLOC, respectively. As a community, we could require papers to report code size in dLOC rather than raw LOC. I believe that would disincentivize researchers from relying on marginal LOC differences (say, less than 5 dLOCs) just to provide the aesthetics of quantitative evaluation to their paper.</span></p>
<p><span style="font-weight: 400">More generally, I want researchers (especially those with the experience and capacity to run user studies) to both explore more usability metrics beyond LOC, and to </span><i><span style="font-weight: 400">validate</span></i><span style="font-weight: 400"> those metrics. Validation is the concept in psychometrics of demonstrating whether a metric is a useful proxy for some human attribute. For instance, the previously cited fMRI study argues that LOC is not a particularly valid metric for either subjective or objective cognitive complexity.</span></p>
<p><span style="font-weight: 400">Consider another proposal. Rather than comparing the size of two programs, what if we compared the size of the </span><i><span style="font-weight: 400">argument that the programs do what they’re supposed to?</span></i><span style="font-weight: 400"> Imagine running a user study where you take a bunch of specifications, implement each spec in several programming models embedded in a proof assistant, then formally prove that each program in each model implements its spec. I conjecture that the length of the proof would have a better correlation with cognitive complexity than the length of the program. If you ran a study showing that’s true, then you’ve now produced a validated metric that other researchers can use </span><i><span style="font-weight: 400">without needing to run a user study on their own</span></i><span style="font-weight: 400">. That is one way which human-centered PL researchers can produce evaluation methods useful to the rest of the community.</span></p>
<p>For further inspiration, here are some creative metrics for system usability I&#8217;ve seen in prior work:</p>
<ul>
<li>&#8220;Automatically Scheduling Halide Image Processing Pipelines&#8221; (<a href="https://dl.acm.org/doi/abs/10.1145/2897824.2925952">Mullapudi et al. 2016</a>) evaluates a Halide auto-scheduler by measuring the time it takes for two Halide experts to construct a schedule of equivalent or greater performance than the automated algorithm.</li>
<li>&#8220;How Profilers Can Help Navigate Type Migration&#8221; (<a href="https://dl.acm.org/doi/10.1145/3622817">Greenman et al. 2023</a>) compares strategies for optimizing gradually-typed codebases by running thousands of simulations of programmers engaging in random variations on each strategy.</li>
<li>&#8220;Catala: A Programming Language for the Law&#8221; (<a href="https://dl.acm.org/doi/10.1145/3473582">Merigoux et al. 2021</a>) evaluates the perceived accessibility of their language to legal experts by repeatedly measuring participant confidence at different stages of a training session.</li>
</ul>
<h2><span style="font-weight: 400">Calls to Action</span></h2>
<p><b>If you want to make a claim about usability in your paper</b><span style="font-weight: 400">: Challenge yourself to be as specific as possible, and don’t stop at “usability.” Read the <a href="https://www.sciencedirect.com/science/article/abs/pii/S1045926X96900099">cognitive dimensions paper</a></span><span style="font-weight: 400">. Invent your own terms!</span></p>
<p><b>If you want to evaluate a claim about usability in your paper: </b><span style="font-weight: 400">Before you reach for lines of code, consider making a strong qualitative argument. Use the specificity of your argument to inspire better metrics than lines of code. But if you really want to use lines of code, consider a logarithmic scale (dLOC).</span></p>
<p><b>If you are reviewing a paper that makes a usability claim: </b><span style="font-weight: 400">Be demanding on the specifics. Don’t let papers get away with broad-brush statements like “Python makes something usable”. But be fair on the evaluation. Don’t just demand a user study or a lines-of-code comparison because that seems rigorous.</span></p>
<p><b>If you want to do research to improve usability evaluation: </b><span style="font-weight: 400">my inbox is always open (</span><a href="mailto:will_crichton@brown.edu"><span style="font-weight: 400">will_crichton@brown.edu</span></a><span style="font-weight: 400">), and I am recruiting PhD students (</span><a href="https://cel.cs.brown.edu/"><span style="font-weight: 400">https://cel.cs.brown.edu/</span></a><span style="font-weight: 400">)!</span></p>
<p><strong>About the author: </strong><em><a href="https://willcrichton.net/">Will Crichton</a> is an incoming assistant professor at Brown University. His research combines programming language theory and cognitive psychology to develop a science of the human factors of programming. Will received his PhD from Stanford University in 2022 advised by Pat Hanrahan and Maneesh Agrawala, and he was a postdoc with Shriram Krishnamurthi before starting as a professor at Brown.</em></p>
<p><strong>Disclaimer:</strong> <em>These posts are written by individual contributors to share their thoughts on the SIGPLAN blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGPLAN or its parent organization, ACM.</em></p>
